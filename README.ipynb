{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeiT: Data-efficient Image Transformers\n",
    "**Transformers go brum brum**\n",
    "\n",
    "Hi guys! Today we are going to implement [Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877) a new method to perform knoledge distiallion on Vision Transformers called DeiT.\n",
    "\n",
    "You will soon see how elegant and simple this new approach is.\n",
    "\n",
    "\n",
    "Code is [here](https://github.com/FrancescoSaverioZuppichini/DeiT), an interactive version of this article can be downloaded from [here](https://github.com/FrancescoSaverioZuppichini/DeiT/blob/main/README.ipynb).\n",
    "\n",
    "DeiT is available on my new computer vision library called [glasses](https://github.com/FrancescoSaverioZuppichini/glasses)\n",
    "\n",
    "Before starting I **highly** reccomand to first have a look at [Vision Transformers](https://towardsdatascience.com/implementing-visualttransformer-in-pytorch-184f9f16f632)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Let's introduce the DeiT models family by having a look at the performance\n",
    "\n",
    "![alt](https://github.com/FrancescoSaverioZuppichini/DeiT/blob/main/images/DeiTTable.png?raw=true)\n",
    "\n",
    "Focus your attention on *ViT-B* and *DeiT-S*. As you can see, their smallest model has + 4-5% and is 100x faster than the bigger *ViT-B*. **How it is possible?**\n",
    "\n",
    "### Knowledge Distillation\n",
    "\n",
    "(The paper as a very good summary section about this topic, I will go fast)\n",
    "\n",
    "Knowledge Distillation is a tranining technique to teach a *student* models using a *teacher* model. This is usually used to, starting from a big model as a *teacher*, create a new smaller *student* model yielding better performance than traning the *student* model by scratch. \n",
    "\n",
    "There different types of distillation technique, in this paper they used what is called Hard-label distillation. The idea is to use both the real target $y$ and the target produced by the *teacher* $y_t=\\text{argmax}_cZ_t(c)$. Where $Z_s$ and $Z_t$ are the logits of the student and teacher model respectively, $\\psi$ is the sofmax function.\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text {global }}^{\\text {hardDistill }}=\\frac{1}{2} \\mathcal{L}_{\\mathrm{CE}}\\left(\\psi\\left(Z_{s}\\right), y\\right)+\\frac{1}{2} \\mathcal{L}_{\\mathrm{CE}}\\left(\\psi\\left(Z_{s}\\right), y_{\\mathrm{t}}\\right)\n",
    "$$\n",
    "\n",
    "Basically the loss will penalise the student when it missclassify real target and teacher target. This is important, because they are not always the same. The teacher could have made some mistake or the picture may have been augmented heavily and thus the target has changed.\n",
    "\n",
    "Interesting, best results were archived when they used a convnet ([regnet](https://arxiv.org/abs/2003.13678)) as teacher not a transformer.\n",
    "\n",
    "![alt](https://github.com/FrancescoSaverioZuppichini/DeiT/blob/main/images/DistillationTableHard.png?raw=true)\n",
    "\n",
    "It is called **hard** because the student depends on the hard labels of the teacher. In PyTorch this can be implemented by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "class HardDistillationLoss(nn.Module):\n",
    "    def __init__(self, teacher: nn.Module):\n",
    "        super().__init__()\n",
    "        self.teacher = teacher\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, inputs: Tensor, outputs : Tensor, labels: Tensor) -> Tensor:\n",
    "        \n",
    "        base_loss = self.criterion(outputs, labels)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = self.teacher(inputs)\n",
    "        teacher_labels = torch.argmax(teacher_outputs, dim=1)\n",
    "        teacher_loss = self.criterion(outputs, teacher_labels)\n",
    "        \n",
    "        return 0.5 * base_loss + 0.5 * teacher_loss\n",
    "    \n",
    "# little test   \n",
    "loss = HardDistillationLoss(nn.Linear(100, 10))\n",
    "_ = loss(torch.rand((8, 100)), torch.rand((8, 10)), torch.ones(8).long())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Distillation\n",
    "\n",
    "![alt](https://github.com/FrancescoSaverioZuppichini/DeiT/blob/main/images/DistillationAttention.png?raw=true)\n",
    "\n",
    "ViT uses the **class token** to make its final prediction. Similary, we can add a **distillation token** than is used to make a second prediction; this second prediction is in the second part of the loss. The authors reported than the class and distillation token converges to very similar vector, as expected because the teacher prediction are similar to the targets, but still not identical.\n",
    "\n",
    "We can easily modify our loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "class HardDistillationLoss(nn.Module):\n",
    "    def __init__(self, teacher: nn.Module):\n",
    "        super().__init__()\n",
    "        self.teacher = teacher\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, inputs: Tensor, outputs: Union[Tensor, Tensor], labels: Tensor) -> Tensor:\n",
    "        # outputs contains booth predictions, one with the cls token and one with the dist token\n",
    "        outputs_cls, outputs_dist = outputs\n",
    "        base_loss = self.criterion(outputs_cls, labels)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = self.teacher(inputs)\n",
    "        teacher_labels = torch.argmax(teacher_outputs, dim=1)\n",
    "        teacher_loss = self.criterion(outputs_dist, teacher_labels)\n",
    "        \n",
    "        return 0.5 * base_loss + 0.5 * teacher_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy!\n",
    "\n",
    "\n",
    "### Distillation Token\n",
    "\n",
    "Now we have to add the `dist` token to our model. DeiT is just a normal ViT with this additional token, so I can recycle the code from my [ViT Tutorial](https://towardsdatascience.com/implementing-visualttransformer-in-pytorch-184f9f16f632). \n",
    "\n",
    "This special token, from which we are going to make the prediciton used in the right part of the loss, is added after the normal classification one. So after we have projected the patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels: int = 3, patch_size: int = 16, emb_size: int = 768, img_size: int = 224):\n",
    "        self.patch_size = patch_size\n",
    "        super().__init__()\n",
    "        self.projection = nn.Sequential(\n",
    "            # using a conv layer instead of a linear one -> performance gains\n",
    "            nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size),\n",
    "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
    "        )\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1, emb_size))\n",
    "        # distillation token\n",
    "        self.dist_token = nn.Parameter(torch.randn(1,1, emb_size))\n",
    "        self.positions = nn.Parameter(torch.randn((img_size // patch_size) **2 + 1, emb_size))\n",
    "\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        b, _, _, _ = x.shape\n",
    "        x = self.projection(x)\n",
    "        cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)\n",
    "        dist_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)\n",
    "        # prepend the cls token to the input\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        # add position embedding\n",
    "        x += self.positions\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Head\n",
    "\n",
    "We also have to change the head to return both predictions at training time. At test time we just average them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, emb_size: int = 768, n_classes: int = 1000):       \n",
    "        super().__init__()\n",
    "\n",
    "        self.head = nn.Linear(emb_size, n_classes)\n",
    "        self.dist_head = nn.Linear(emb_size, n_classes)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x, x_dist = x[:, 0], x[:, 1]\n",
    "        x_head = self.head(x)\n",
    "        x_dist_head = self.dist_head(x_dist)\n",
    "        \n",
    "        if self.training:\n",
    "            x = x_head, x_dist_head\n",
    "        else:\n",
    "            x = (x_head + x_dist_head) / 2\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, it follows the same ViT code I used in my [previous article](https://towardsdatascience.com/implementing-visualttransformer-in-pytorch-184f9f16f632)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size: int = 768, num_heads: int = 8, dropout: float = 0):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        # fuse the queries, keys and values in one matrix\n",
    "        self.qkv = nn.Linear(emb_size, emb_size * 3)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "        \n",
    "    def forward(self, x : Tensor, mask: Tensor = None) -> Tensor:\n",
    "        # split keys, queries and values in num_heads\n",
    "        qkv = rearrange(self.qkv(x), \"b n (h d qkv) -> (qkv) b h n d\", h=self.num_heads, qkv=3)\n",
    "        queries, keys, values = qkv[0], qkv[1], qkv[2]\n",
    "        # sum up over the last axis\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys) # batch, num_heads, query_len, key_len\n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            energy.mask_fill(~mask, fill_value)\n",
    "            \n",
    "        scaling = self.emb_size ** (1/2)\n",
    "        att = F.softmax(energy, dim=-1) / scaling\n",
    "        att = self.att_drop(att)\n",
    "        # sum up over the third axis\n",
    "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "    \n",
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        \n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x\n",
    "    \n",
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size: int, expansion: int = 4, drop_p: float = 0.):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )\n",
    "        \n",
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 emb_size: int = 768,\n",
    "                 drop_p: float = 0.,\n",
    "                 forward_expansion: int = 4,\n",
    "                 forward_drop_p: float = 0.,\n",
    "                 ** kwargs):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                MultiHeadAttention(emb_size, **kwargs),\n",
    "                nn.Dropout(drop_p)\n",
    "            )),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                FeedForwardBlock(\n",
    "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )\n",
    "            ))\n",
    "\n",
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, depth: int = 12, **kwargs):\n",
    "        super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally our model looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeiT(nn.Sequential):\n",
    "    def __init__(self,     \n",
    "                in_channels: int = 3,\n",
    "                patch_size: int = 16,\n",
    "                emb_size: int = 768,\n",
    "                img_size: int = 224,\n",
    "                depth: int = 12,\n",
    "                n_classes: int = 1000,\n",
    "                **kwargs):\n",
    "        super().__init__(\n",
    "            PatchEmbedding(in_channels, patch_size, emb_size, img_size),\n",
    "            TransformerEncoder(depth, emb_size=emb_size, **kwargs),\n",
    "            ClassificationHead(emb_size, n_classes)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
